{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, track frequency of tracks and record descriptive track info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_counts = collections.Counter()\n",
    "track_info = {}\n",
    "\n",
    "data_path = '/Users/kwu/data/recsys2018/mpd.v1/data'\n",
    "\n",
    "for fname in os.listdir(data_path):\n",
    "    data = json.loads(open(os.path.join(data_path, fname)).read())\n",
    "    \n",
    "    for playlist in data['playlists']:\n",
    "        for track in playlist['tracks']:\n",
    "            track_id = track['track_uri'].split(':')[-1]\n",
    "            track_counts[track_id] += 1  \n",
    "            track_info[track_id] = {\n",
    "                            'track_name': track['track_name'],\n",
    "                            'artist_name': track['artist_name'],\n",
    "                            'album_name': track['album_name']\n",
    "                        }\n",
    "            \n",
    "with open('track_counts.json', 'w') as f:\n",
    "    json.dump(dict(track_counts), f)\n",
    "    \n",
    "with open('track_info_dict.json', 'w') as f:\n",
    "    json.dump(track_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load track count info. Filter out infrequent tracks (below track_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_counts = collections.Counter(json.loads(open('../data/track_counts.json').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_threshold = 100\n",
    "common_track_counts =  collections.Counter({k:c for k, c in track_counts.items() if c > track_threshold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69678"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_track_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping of Spotify track ids to internal integer mapping (from 1 to M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_track_ids = common_track_counts.keys()\n",
    "track_id_map = dict(zip(valid_track_ids, range(len(valid_track_ids))))\n",
    "rev_track_id_map = dict(zip(track_id_map.values(), track_id_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Parse data again, keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 412.9s\n"
     ]
    }
   ],
   "source": [
    "playlist_threshold = 50\n",
    "playlist_count = 0\n",
    "\n",
    "playlist_id_map = {}\n",
    "\n",
    "row_inds = []\n",
    "col_inds = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "valid_track_id_set = set(valid_track_ids)\n",
    "\n",
    "for fname in os.listdir(data_path):\n",
    "    data = json.loads(open(os.path.join(data_path, fname)).read())\n",
    "    for playlist in data['playlists']:\n",
    "        valid_tracks = [track_id_map[track['track_uri'].split(':')[-1]] for track in playlist['tracks'] if track['track_uri'].split(':')[-1] in valid_track_id_set]\n",
    "        if len(valid_tracks) > playlist_threshold:\n",
    "            \n",
    "            playlist_id_map[playlist_count] = playlist['pid']\n",
    "            row_inds += [playlist_count] * len(valid_tracks)\n",
    "            col_inds += valid_tracks\n",
    "            \n",
    "            playlist_count += 1\n",
    "            \n",
    "end = time.time()\n",
    "print (\"Time elapsed: {}s\".format(round(end-start, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_thin = np.array([row_inds, col_inds]).T\n",
    "df = pd.DataFrame(data_thin)\n",
    "df.columns = ['user_id', 'track_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2LQIXDbSokfmms0rlffgXq', '4EezDPRZTcFd5iFZwvAOL4', '6MGrXTz0TZaO6sbHZhOq61']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks_to_delete = set(rev_track_id_map.keys()) - set(df['track_id'].unique())\n",
    "tracks_to_delete\n",
    "bad_spotify_track_ids = [rev_track_id_map[track] for track in tracks_to_delete]\n",
    "bad_spotify_track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_ids = df['track_id'].unique()\n",
    "\n",
    "new_valid_track_ids = [rev_track_id_map[track_id] for track_id in remaining_ids]\n",
    "new_track_id_map = dict(zip(new_valid_track_ids, range(len(new_valid_track_ids))))\n",
    "new_rev_track_id_map = dict(zip(new_track_id_map.values(), new_track_id_map.keys()))\n",
    "\n",
    "old_new_map = dict(zip(remaining_ids, new_rev_track_id_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['track_id'] = df['track_id'].apply(lambda x: old_new_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_inds = np.array(df['user_id'])\n",
    "col_inds = np.array(df['track_id'])\n",
    "data = sparse.coo_matrix(([1] * len(row_inds), (row_inds, col_inds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sparse.coo_matrix(([1] * len(row_inds), (row_inds, col_inds)))\n",
    "sparse.save_npz(\"raw_sparse_2.npz\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('raw_df_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from sparse .npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sparse.load_npz('../data/raw_sparse_2.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'user_id': data.row, 'track_id': data.col})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dawenl/vae_cf/blob/master/VAE_ML20M_WWW2018.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user_id')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 39106490 playlist inclusion events from 393740 playlist and 69675 tracks (sparsity: 0.143%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. * df.shape[0] / (data.shape[0] * data.shape[1])\n",
    "\n",
    "print(\"After filtering, there are %d playlist inclusion events from %d playlist and %d tracks (sparsity: %.3f%%)\" % \n",
    "      (df.shape[0], data.shape[0], data.shape[1], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_uid = pd.unique(df['user_id'])\n",
    "\n",
    "n_users = data.shape[0]\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_plays = df.loc[df['user_id'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sid = pd.unique(train_plays['track_id'])\n",
    "len(unique_sid)\n",
    "### THIS HAS TO MATCH THE ORIGINAL NUMBER OF UNIQUE TRACKS -- OTHERWISE RESAMPLE TRAINING USER IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69675"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(df['track_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/kjw2157/6998-Adv-ML-Final-Proj/mpd_proc'\n",
    "pro_dir = os.path.join(DATA_DIR, 'data_small_2')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user_id')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays = df.loc[df['user_id'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['track_id'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays = df.loc[df['user_id'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['track_id'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['user_id']))\n",
    "    sid = list(map(lambda x: show2id[x], tp['track_id']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "train_data = numerize(train_plays)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
