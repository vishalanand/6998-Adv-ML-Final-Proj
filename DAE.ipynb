{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Activation, Dense, Input, Flatten, Dropout, Lambda, Softmax\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_sid = list()\n",
    "with open(os.path.join('pro_sg', 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('pro_sg/train.csv')\n",
    "n_users = df['uid'].max() + 1\n",
    "\n",
    "rows, cols = df['uid'], df['sid']\n",
    "train = sparse.csr_matrix((np.ones_like(rows),\n",
    "                         (rows, cols)), dtype='float64',\n",
    "                         shape=(n_users, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_train, valid_test = load_tr_te_data('pro_sg/validation_tr.csv', 'pro_sg/validation_te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 69675)             0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 69675)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 69675)             0         \n",
      "_________________________________________________________________\n",
      "latent1_enc (Dense)          (None, 200)               13935200  \n",
      "=================================================================\n",
      "Total params: 13,935,200\n",
      "Trainable params: 13,935,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "h1 = 200\n",
    "\n",
    "enc_inputs = Input(shape=(n_items,), name='enc_input')\n",
    "inputs_normed = Lambda(lambda  x: tf.nn.l2_normalize(x, dim=1))(enc_inputs)\n",
    "x = Dropout(0.5)(inputs_normed)\n",
    "\n",
    "latent1_enc = Dense(h1, activation='tanh', name='latent1_enc', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "\n",
    "encoder = Model(enc_inputs, latent1_enc, name='encoder')\n",
    "encoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dec_inputs (InputLayer)      (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "logits_dec (Dense)           (None, 69675)             14004675  \n",
      "_________________________________________________________________\n",
      "probs_dec (Softmax)          (None, 69675)             0         \n",
      "=================================================================\n",
      "Total params: 14,004,675\n",
      "Trainable params: 14,004,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec_inputs = Input(shape=(h1,), name='dec_inputs')\n",
    "\n",
    "logits = Dense(n_items, name='logits_dec', kernel_regularizer=regularizers.l2(0.01))(dec_inputs)\n",
    "probs = Softmax(name='probs_dec')(logits) \n",
    "decoder = Model(dec_inputs, probs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 69675)             0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 200)               13935200  \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 69675)             14004675  \n",
      "=================================================================\n",
      "Total params: 27,939,875\n",
      "Trainable params: 27,939,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def customLoss(yTrue,yPred):\n",
    "    return -K.mean(K.sum(yTrue * K.log(yPred))\n",
    "\n",
    "\n",
    "dae = Model(inputs=enc_inputs, outputs=decoder(encoder(enc_inputs)), name='autoencoder')\n",
    "dae.summary()\n",
    "\n",
    "dae.compile(loss=customLoss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def data_generator(data, batch_size):\n",
    "    N = data.shape[0]\n",
    "    while True:\n",
    "        roworder = np.random.permutation(N)\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(N, start+batch_size)\n",
    "            batch = data[roworder[start:end]]\n",
    "            batch = batch.toarray().astype('float32')\n",
    "            yield (batch, batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class RecMetrics(Callback):\n",
    "    \n",
    "    def __init__(self, val_train_data, val_test_data):\n",
    "        self.val_train_data = val_train_data\n",
    "        self.val_test_data = val_test_data\n",
    "        return\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.ndcg_hist = []\n",
    "        self.recall_hist = []\n",
    "        return\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        N = self.val_train_data.shape[0]\n",
    "        ndcg_list = []\n",
    "        for start in range(0, N, 500):\n",
    "            end = min(N, start+500)\n",
    "            X = self.val_train_data[start:end]\n",
    "            X = X.toarray().astype('float32')\n",
    "            preds = np.asarray(self.model.predict(X))\n",
    "            preds[X.nonzero()] = -np.inf\n",
    "            ndcg_list.append(NDCG_binary_at_k_batch(preds, self.val_test_data[start:end]))\n",
    "        print (\"NDCG: {}\".format(np.mean(ndcg_list)))\n",
    "        \n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "    \n",
    "rec_metrics = RecMetrics(valid_train, valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "747/747 [==============================] - 177s 237ms/step - loss: 413200.7663\n",
      "NDCG: 0.2694218756511447\n",
      "Epoch 2/30\n",
      "747/747 [==============================] - 173s 232ms/step - loss: 373939.6806\n",
      "NDCG: 0.3037702374050533\n",
      "Epoch 3/30\n",
      "747/747 [==============================] - 173s 232ms/step - loss: 366818.0267\n",
      "NDCG: 0.32158573017953457\n",
      "Epoch 4/30\n",
      "692/747 [==========================>...] - ETA: 12s - loss: 362601.1870"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "dae.fit_generator(train_generator(train, 500),\n",
    "                 verbose=1, \n",
    "                 epochs=30,\n",
    "                 steps_per_epoch = train.shape[0]//500,\n",
    "                 callbacks=[rec_metrics])\n",
    "# dae.fit(x=train, y=train,\n",
    "#         batch_size = 500,\n",
    "#         verbose=2,\n",
    "#         epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
